<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>Neurosynth Compose Docs Blog</title>
        <link>https://neurostuff.github.io/compose-docs/blog</link>
        <description>Neurosynth Compose Docs Blog</description>
        <lastBuildDate>Tue, 02 Jan 2024 00:00:00 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[New Year Updates]]></title>
            <link>https://neurostuff.github.io/compose-docs/blog/2024/1/2/new-year</link>
            <guid>https://neurostuff.github.io/compose-docs/blog/2024/1/2/new-year</guid>
            <pubDate>Tue, 02 Jan 2024 00:00:00 GMT</pubDate>
            <description><![CDATA[Hello Neurosynth Users,]]></description>
            <content:encoded><![CDATA[<p>Hello Neurosynth Users,</p><p>Happy New Year! </p><p>2023 was a very exciting year for Neurosynth, having launched our Compose platform to the public and announced it on social media. In the December we‚Äôve saw <strong>over 500 new user visits</strong>, with <strong>200 users signing up for an account</strong>! üöÄ</p><p>Help us keep this growth going by <a href="/compose-docs/blog/2024/1/2/blog/announcing-ns-compose">sharing our announcement</a> with your colleagues. üßë‚Äçüî¨</p><h1>üåü What‚Äôs New üåü</h1><p>We‚Äôve also continued to introduce new features and improve the user experience. Here‚Äôs some highlights:</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="large-scale-association-tests">Large-scale association tests<a href="#large-scale-association-tests" class="hash-link" aria-label="Direct link to Large-scale association tests" title="Direct link to Large-scale association tests">‚Äã</a></h3><p>A key feature that set Neurosynth aside were large-scale association maps (previously known as ‚Äúreverse inference‚Äù).</p><p>Whereas a typical meta-analysis tells you if activity is consistently reported in a target set of studies, an association test tells you if <strong>activation occurs more consistently in this set of studies versus a large and diverse reference sample</strong>. </p><p>That's important, because this allows you to control for base rate differences between regions. Certain regions, such as the insula or lateral PFC for instance, play a very broad role in cognition, and hence are consistently activated for many different tasks and cognitive states. Thus, if you see insula activity in your meta-analysis, you might erroneously conclude that the insula is involved in the cognitive state you're studying. A large-scale association test lets you determine if the activity you observe in a region occurs <em>more consistently</em> in your meta-analysis than in other studies, making it possible to make more confident claims that a given region is involved in a particular process, and isn't involved in just about every task.</p><p>Previously association tests were available for the automatically generated maps on neurosynth.org. <strong>Now you can perform large-scale association tests for your custom meta-analyses in Neurosynth Compose.</strong></p><p>We have created a full primer and tutorial on MKDA Chi-Squared, including an example from a recent meta-analysis on social processing. Check it out!</p><style data-emotion="css 1hxq67s">.css-1hxq67s{font-family:"Roboto","Helvetica","Arial",sans-serif;font-weight:500;font-size:0.875rem;line-height:1.75;letter-spacing:0.02857em;text-transform:uppercase;min-width:64px;padding:6px 16px;border-radius:4px;-webkit-transition:background-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,box-shadow 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,border-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;transition:background-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,box-shadow 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,border-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;color:#fff;background-color:#1976d2;box-shadow:0px 3px 1px -2px rgba(0,0,0,0.2),0px 2px 2px 0px rgba(0,0,0,0.14),0px 1px 5px 0px rgba(0,0,0,0.12);}.css-1hxq67s:hover{-webkit-text-decoration:none;text-decoration:none;background-color:#1565c0;box-shadow:0px 2px 4px -1px rgba(0,0,0,0.2),0px 4px 5px 0px rgba(0,0,0,0.14),0px 1px 10px 0px rgba(0,0,0,0.12);}@media (hover: none){.css-1hxq67s:hover{background-color:#1976d2;}}.css-1hxq67s:active{box-shadow:0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);}.css-1hxq67s.Mui-focusVisible{box-shadow:0px 3px 5px -1px rgba(0,0,0,0.2),0px 6px 10px 0px rgba(0,0,0,0.14),0px 1px 18px 0px rgba(0,0,0,0.12);}.css-1hxq67s.Mui-disabled{color:rgba(0, 0, 0, 0.26);box-shadow:none;background-color:rgba(0, 0, 0, 0.12);}</style><style data-emotion="css 1hw9j7s">.css-1hw9j7s{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;position:relative;box-sizing:border-box;-webkit-tap-highlight-color:transparent;background-color:transparent;outline:0;border:0;margin:0;border-radius:0;padding:0;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;vertical-align:middle;-moz-appearance:none;-webkit-appearance:none;-webkit-text-decoration:none;text-decoration:none;color:inherit;font-family:"Roboto","Helvetica","Arial",sans-serif;font-weight:500;font-size:0.875rem;line-height:1.75;letter-spacing:0.02857em;text-transform:uppercase;min-width:64px;padding:6px 16px;border-radius:4px;-webkit-transition:background-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,box-shadow 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,border-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;transition:background-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,box-shadow 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,border-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;color:#fff;background-color:#1976d2;box-shadow:0px 3px 1px -2px rgba(0,0,0,0.2),0px 2px 2px 0px rgba(0,0,0,0.14),0px 1px 5px 0px rgba(0,0,0,0.12);}.css-1hw9j7s::-moz-focus-inner{border-style:none;}.css-1hw9j7s.Mui-disabled{pointer-events:none;cursor:default;}@media print{.css-1hw9j7s{-webkit-print-color-adjust:exact;color-adjust:exact;}}.css-1hw9j7s:hover{-webkit-text-decoration:none;text-decoration:none;background-color:#1565c0;box-shadow:0px 2px 4px -1px rgba(0,0,0,0.2),0px 4px 5px 0px rgba(0,0,0,0.14),0px 1px 10px 0px rgba(0,0,0,0.12);}@media (hover: none){.css-1hw9j7s:hover{background-color:#1976d2;}}.css-1hw9j7s:active{box-shadow:0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);}.css-1hw9j7s.Mui-focusVisible{box-shadow:0px 3px 5px -1px rgba(0,0,0,0.2),0px 6px 10px 0px rgba(0,0,0,0.14),0px 1px 18px 0px rgba(0,0,0,0.12);}.css-1hw9j7s.Mui-disabled{color:rgba(0, 0, 0, 0.26);box-shadow:none;background-color:rgba(0, 0, 0, 0.12);}</style><a class="MuiButtonBase-root MuiButton-root MuiButton-contained MuiButton-containedPrimary MuiButton-sizeMedium MuiButton-containedSizeMedium MuiButton-root MuiButton-contained MuiButton-containedPrimary MuiButton-sizeMedium MuiButton-containedSizeMedium css-1hw9j7s" tabindex="0" href="tutorial/advanced/mkda_association">MKDA Chi-Squared Tutorial üßë‚Äçüéì</a><h3 class="anchor anchorWithStickyNavbar_LWe7" id="ux-enhancements-">UX Enhancements ‚ú®<a href="#ux-enhancements-" class="hash-link" aria-label="Direct link to UX Enhancements ‚ú®" title="Direct link to UX Enhancements ‚ú®">‚Äã</a></h3><p>Based on your valuable feedback, we've made numerous bug fixes and improvements: </p><ul><li><p><strong>Simplified Curation</strong>: The review import page has been removed, and summary information is now added directly to the tag step.</p></li><li><p><strong>Searching UI</strong>: We've replaced the dropdown with a selection gallery, making it easier to choose your preferred search method, and we now auto-generate search import names. In addition, resolving duplicates is skipped if none are present. </p></li><li><p><strong>Improved Editing Workflow</strong>: The editing interface has been improved, streamlining the extraction process. </p></li><li><p><strong>Various UX Improvements and Fixes</strong>: We fixed many papercuts, especially in the <em>Extraction</em> phase.</p></li></ul><p>We hope you enjoy these changes.</p><p>Email us any <a href="mailto:neurosynthorg@gmail.com" target="_blank" rel="noopener noreferrer">feedback</a>, or ask a question on <a href="https://neurostars.org/tag/neurosynth-compose" target="_blank" rel="noopener noreferrer">NeuroStars</a> if you have issues.</p><p>Cheers,</p><p>The Neurosynth Team üß†</p>]]></content:encoded>
            <category>neurosynth</category>
        </item>
        <item>
            <title><![CDATA[New tutorials and updates]]></title>
            <link>https://neurostuff.github.io/compose-docs/blog/tutorials-updates</link>
            <guid>https://neurostuff.github.io/compose-docs/blog/tutorials-updates</guid>
            <pubDate>Tue, 28 Nov 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[Dear Neurosynth Community,]]></description>
            <content:encoded><![CDATA[<p>Dear Neurosynth Community,</p><p>I'm excited to announce important updates to <em>Neurosynth Compose</em>: A free and open platform for neuroimaging meta-analysis.</p><p>First, we have added some easy to follow <a href="https://neurostuff.github.io/compose-docs/tutorial" target="_blank" rel="noopener noreferrer">tutorials</a> to our documentation, making it easy to become familiar with our platform. </p><p>The tutorials cover two main uses cases we support: Manual and Automated Meta-analyses. Our platform make gold-standard <em>manual</em> meta-analyses much easier, by leveraging pre-extracted imaging data
and streamline user interfaces. Automated meta-analyses are ideal for generating exploratory results rapidly, enabling meta-analysis as part of routine scientific practice.  </p><p>We've also made many small but important updates to our platform, including significant performance updates and improvements to the user interface.
<em>Neurosynth Compose</em> is now more intuitive and easier to use. Give it a try by following our <a href="https://neurostuff.github.io/compose-docs/tutorial/manual" target="_blank" rel="noopener noreferrer">manual meta-analysis tutorial</a>. </p><p>We also have some exciting new features in the pipeline that we'll release in early 2024 including:</p><ul><li>Image-based Meta-Analysis (IBMA). Soon, you will be able to use NeuroVault data as inputs for IBMA-- a more powerful and sensitive alternative to Coordinate Based Meta-Analysis.</li><li>Advanced data extraction using Large Language Models (GPT). Early protypes to extract detailed information (such as participant demographics) from neuroimaging articles using LLMs
have shown promise. We are working on incorporating these workflows into <em>Neurosynth Compose</em>, making it even easier to identify relevant studies for meta-analysis.</li></ul><p>We look forward to your feedback!</p><p>-Alejandro</p>]]></content:encoded>
            <category>hello</category>
            <category>neurosynth</category>
        </item>
        <item>
            <title><![CDATA[Announcing Neurosynth Compose!]]></title>
            <link>https://neurostuff.github.io/compose-docs/blog/announcing-ns-compose</link>
            <guid>https://neurostuff.github.io/compose-docs/blog/announcing-ns-compose</guid>
            <pubDate>Sun, 13 Aug 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[Dear Neurosynth Community,]]></description>
            <content:encoded><![CDATA[<p>Dear Neurosynth Community,</p><p>My name is Alejandro, and I am the current project leader of the Neurosynth project.</p><p>I am very excited to announce to you that the Neurosynth project lives on, and we are officially announcing the (beta) release of the latest member of the ecosystem: Neurosynth Compose.</p><p><em>Neurosynth Compose</em> enables users to easily perform custom neuroimaging meta-analyses using a web-based platform, with no programming experience required. This project addresses one of the most commonly request features, which is the ability to customize large-scale meta-analyses using you own expert knowledge.</p><p><em>Neurosynth Compose</em> is <em>free to use</em> and helps users:</p><ul><li>üîé <strong>Search</strong> across over 20,000 studies in the Neurosynth database, or import from external databses such as PubMed.</li><li>üóÉÔ∏è <strong>Curate</strong> your StudySet using systematic review tools conforming to the <a href="https://www.prisma-statement.org/" target="_blank" rel="noopener noreferrer">PRISMA</a> guidelines.</li><li>üìù <strong>Extract</strong> coordinates and metadata for each study, leveraging thousands of pre-extracted studies to minimize effort.</li><li>üìä <strong>Analyze</strong> by specifying a reproducible <a href="https://readthedocs.org/projects/nimare/" target="_blank" rel="noopener noreferrer">NiMARE</a> workflow, and execute it locally or in the cloud.</li><li>üîó <strong>Share</strong> with the community with complete provenance and reproducibility.</li></ul><p>The goal of <em>Neurosynth Compose</em> is to enable researchers to go beyond the finite set of automatically generated meta-analyses from the original platform and overcome limitations from automated coordinate and semantic extraction. The end result is a gold standard meta-analysis, in much less time than a manual workflow, and with much greater reproducible. </p><p>Currently, <em>Neurosynth Compose</em> is in beta, and under active development. We welcome feedback to ensure our platform meets the needs of the community. Please leave us feedback using the button on the bottom right corner of the screen!</p><p>We are working on several upcoming features that will make the platform even better. Many of these features are already available in our Python meta-analysis library, NiMARE, and we are actively working on the user facing online interfaces.</p><ul><li><strong>Image-based Meta-Analysis (IBMA)</strong>. We have developed algorithms in NiMARE for using whole-brain statistical maps as inputs to meta-analysis. This is more powerful and sensitive technique compared to Coordinate-base Meta-Analysis. Soon, you will be able to use NeuroVault data as inputs for your meta-analyses.</li><li><strong>MKDA Chi-squared / Association test</strong>. A hallmark feature of Neurosynth is the ability to relate meta-analytic findings to the rest of the literature, to determine the strength and specificity of an association (this was previously called "reverse inference"). This will soon be possible on your custom meta-analyses.</li><li><strong>A wide range of improvements to the user experience</strong>. We are in the process of re-working parts of the online interface to decrease friction when creating a StudySet, making study utilization, and editing more intuitive. </li></ul><p>I would like to thank everyone involved in this highly-collaborative project, but especially James Kent, a postdoctoral fellow, and Nick Lee, a software engineer, who did the lion's share of the work.</p><p>We are excited for you to try it and let us know what you think.</p><p>-Alejandro</p>]]></content:encoded>
            <category>hello</category>
            <category>neurosynth</category>
        </item>
    </channel>
</rss>