<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.3.1">
<title data-rh="true">AI-Assisted Review | Neurosynth Compose Docs</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://neurostuff.github.io/compose-docs/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://neurostuff.github.io/compose-docs/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://neurostuff.github.io/compose-docs/blog/2025/6/20/ai-review"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="AI-Assisted Review | Neurosynth Compose Docs"><meta data-rh="true" name="description" content="We’re excited to share the release of a significant new innovation in Neurosynth Compose: AI-Assisted Curation. This new feature aims to simplify and accelerate custom neuroimaging meta-analyses, making it easier than ever to curate a set of studies for inclusion into a quantitative meta-analysis."><meta data-rh="true" property="og:description" content="We’re excited to share the release of a significant new innovation in Neurosynth Compose: AI-Assisted Curation. This new feature aims to simplify and accelerate custom neuroimaging meta-analyses, making it easier than ever to curate a set of studies for inclusion into a quantitative meta-analysis."><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2025-06-20T00:00:00.000Z"><meta data-rh="true" property="article:author" content="https://github.com/adelavega"><meta data-rh="true" property="article:tag" content="neurosynth"><link data-rh="true" rel="icon" href="/compose-docs/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://neurostuff.github.io/compose-docs/blog/2025/6/20/ai-review"><link data-rh="true" rel="alternate" href="https://neurostuff.github.io/compose-docs/blog/2025/6/20/ai-review" hreflang="en"><link data-rh="true" rel="alternate" href="https://neurostuff.github.io/compose-docs/blog/2025/6/20/ai-review" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/compose-docs/blog/rss.xml" title="Neurosynth Compose Docs RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/compose-docs/blog/atom.xml" title="Neurosynth Compose Docs Atom Feed"><link rel="stylesheet" href="/compose-docs/assets/css/styles.cce6c2a6.css">
<link rel="preload" href="/compose-docs/assets/js/runtime~main.97dccc22.js" as="script">
<link rel="preload" href="/compose-docs/assets/js/main.559d4219.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/compose-docs/"><div class="navbar__logo"><img src="/compose-docs/img/synth.png" alt="Neurosynth Compose logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/compose-docs/img/synth.png" alt="Neurosynth Compose logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">Neurosynth Compose</b></a><a class="navbar__item navbar__link" href="/compose-docs/">Docs</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/compose-docs/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://compose.neurosynth.org" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Compose Home<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">Recent posts</div><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a aria-current="page" class="sidebarItemLink_mo7H sidebarItemLinkActive_I1ZP" href="/compose-docs/blog/2025/6/20/ai-review">AI-Assisted Review</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/compose-docs/blog/2024/1/31/new-year">New Year, New Features</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/compose-docs/blog/tutorials-updates">New tutorials and updates</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/compose-docs/blog/announcing-ns-compose">Announcing Neurosynth Compose!</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><article itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h1 class="title_f1Hy" itemprop="headline">AI-Assisted Review</h1><div class="container_mt6G margin-vert--md"><time datetime="2025-06-20T00:00:00.000Z" itemprop="datePublished">June 20, 2025</time> · <!-- -->9 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/adelavega" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.com/adelavega.png" alt="Alejandro de la Vega"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/adelavega" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Alejandro de la Vega</span></a></div><small class="avatar__subtitle" itemprop="description">Research Scientist</small></div></div></div></div></header><div id="post-content" class="markdown" itemprop="articleBody"><p>We’re excited to share the release of a significant new innovation in Neurosynth Compose: <strong>AI-Assisted Curation</strong>. This new feature aims to simplify and accelerate custom neuroimaging meta-analyses, making it easier than ever to curate a set of studies for inclusion into a quantitative meta-analysis.</p><p>As Neurosynth Compose users well know, even with the help of a streamlined user interface, manually reviewing studies for inclusion into a meta-analysis is very time-consuming. In fact, a single systematic review can take hundreds of hours, which severely limits the kinds of research questions we can explore. For this very reason, the original Neurosynth platform, which used text-mining to group neuroimaging studies by keywords or topics, was a big step forward. However, the low hanging fruit from these methods have largely been picked. Additionally, crucial details about how a study was done or who participated (like sample size, age, patient group, and experimental specifics) are hard to extract automatically using simple methods such as text frequency or heuristics because they&#x27;re often described inconsistently or not mentioned frequently in the text.</p><p>Here, we aimed to leverage innovations in Zero-Shot Learning using Large Language Models (LLMs) and pair this with our platform for custom meta-analysis (Neurosynth Compose) to <strong>make precise, automated meta-analysis of neuroimaging literature a reality.</strong></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="large-language-models-llms-for-neuroscientific-information-extraction"><strong>Large Language Models (LLMs) for Neuroscientific Information Extraction</strong><a href="#large-language-models-llms-for-neuroscientific-information-extraction" class="hash-link" aria-label="Direct link to large-language-models-llms-for-neuroscientific-information-extraction" title="Direct link to large-language-models-llms-for-neuroscientific-information-extraction">​</a></h2><p>At the heart of this effort is the recent ability for LLMs to understand language with little specialized training. Historically, developing AI models for scientific information extraction was difficult due to the large number of annotated examples required for training. For low-annotation fields like neuroimaging, that largely meant that state-of-the-art biomedical NLP models were out of reach.</p><p>However, recent advancements in <strong>LLM transfer learning</strong> have made it possible to automatically extract information from articles, even in areas where there are very few existing human-labeled examples. Newer LLMs that are trained on vast amounts of general text can be prompted to learn new information with no training data. This approach called &quot;<strong>zero-shot learning</strong>&quot; means SOTA LLMs can extract information even if they haven&#x27;t seen that exact type of task before.</p><p>Here, we use these models to <strong>extract specific details directly from the full text of over 30,000 neuroimaging studies indexed in the NeuroStore database</strong>. By carefully guiding the AI to focus on verifiable facts within the paper, we can reduce the chance of hallucinations, and allow us to verify how accurately key details are extracted. Using this information, we can build a large, structured collection of neuroscientific facts—including participant demographics, study designs, task information, and more—which is then seamlessly presented to you during the curation stage.</p><p><img loading="lazy" alt="Figure 1" src="/compose-docs/assets/images/ai_extraction_workflow-a0d81fd1699210283087eba475584512.png" width="932" height="203" class="img_ev3q">
Figure 1<!-- -->.<!-- --> High-level overview of Zero Shot Information Extraction using LLM prompting</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="a-revamped-curation-experience"><strong>A Revamped Curation Experience</strong><a href="#a-revamped-curation-experience" class="hash-link" aria-label="Direct link to a-revamped-curation-experience" title="Direct link to a-revamped-curation-experience">​</a></h2><p>This information is presented in two key places within the revamped curation user interface: a <strong>concise table view</strong>, allowing for easy comparison across papers, and more <strong>detailed individual study pages</strong> where you can delve deeper into the extracted specifics for a single paper. This dramatically facilitates systematic literature review and helps you efficiently screen studies for eligibility into your meta-analysis research question, all within a fully web-based, PRISMA-compliant workflow (Note that for the PRISMA-workflow AI features are available in the Screening and Eligibility phases).</p><p><img loading="lazy" alt="Figure 2" src="/compose-docs/assets/images/ai_table_view-99630e3c0efd157c2784519f63f5cd84.png" width="1124" height="395" class="img_ev3q">
Figure 2<!-- -->.<!-- --> Table view showing AI-extracted information (Task Name, Group names, Diagnosis), across three studies</p><p>By clicking on a row in the table view, you can see study-level meta-data and extracted features in more detail:</p><p><img loading="lazy" alt="Figure 3" src="/compose-docs/assets/images/ai_study_view-9a2466b58b671400c1f84256bb592f1a.png" width="837" height="612" class="img_ev3q">
Figure 3<!-- -->.<!-- --> Detailed study-evel AI-extracted information, showing Participant Demographics.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="iterative-approach-to-validation-and-development"><strong>Iterative Approach to Validation and Development</strong><a href="#iterative-approach-to-validation-and-development" class="hash-link" aria-label="Direct link to iterative-approach-to-validation-and-development" title="Direct link to iterative-approach-to-validation-and-development">​</a></h2><p>Our approach to information extraction is specifically focused on study details relevant for neuroimaging meta-analysis. We have developed specific extraction schemas that capture the nuanced details crucial for meta-analysis in this field. For each set of guidelines, a sample of studies is manually reviewed and tagged, and the automated extractions are checked for accuracy against these manual tags, both by numbers and by human review. This thorough process makes sure that when new extraction features are introduced to the platform, a reasonable level of accuracy can be established. In contrast with domain-general automated literature review platform and deep review platforms (e.g Elict, Perplexity, Google Notebook LM), the specific extraction schemas have been validation and aligned with expert-guided knowledge representations. </p><p><img loading="lazy" alt="Figure 4" src="/compose-docs/assets/images/iterative_workflow-1fcdb85d034a4dc3907bf91278f0ad22.png" width="432" height="299" class="img_ev3q"></p><p>Figure 4<!-- -->.<!-- --> Iterative annotation and prompt development workflow. </p><p><strong>This effort is an ongoing process that is open to community feedback.</strong> The goal is to continuously refine and check our extraction guidelines for neuroimaging-specific study information that help researchers find and screen studies for inclusion into meta-analysis. This extracted data can then be used with the rest of our existing comprehensive meta-analysis ecosystem (i.e. Neurosynth Compose &amp; NiMARE), to perform detailed meta-analyses. </p><p>All of the studies annotated for Neurosynth Compose are sourced from PubMed Central using <em>pubget</em> and annotated using <em>labelbuddy</em>— a set of tools our group recently introduced by our group for literature mining (Dockes et al., 2024). All of the annotations we have generated are openly accessible under the <a href="https://github.com/litmining/labelbuddy-annotations/" target="_blank" rel="noopener noreferrer">labelbuddy annotations GitHub repository</a></p><p>The extraction pipelines that are validated and iteratively developed using these annotations (and put into production on Neurosynth Compose), are also <a href="https://github.com/neurostuff/neurostore-text-extraction/tree/main/ns_extract/pipelines" target="_blank" rel="noopener noreferrer">openly available</a>.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="initial-extraction-schemas"><strong>Initial Extraction Schemas</strong><a href="#initial-extraction-schemas" class="hash-link" aria-label="Direct link to initial-extraction-schemas" title="Direct link to initial-extraction-schemas">​</a></h2><p>At launch, we have extracted two schemas across the articles indexed by NeuroStore: <em>participant demographics</em> and <em>experimental details.</em> To begin, these schemas were extracted from the full text of articles using GPT-4— a model we previously established performed well at information extraction. </p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="participant-demographics"><strong>Participant Demographics</strong><a href="#participant-demographics" class="hash-link" aria-label="Direct link to participant-demographics" title="Direct link to participant-demographics">​</a></h3><p>Participant demographics were extracted for each experimental group in the study. LLM models were instructed to focus on values that were <em>explicitly</em> mentioned in the text</p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>count</code></td><td>Total participants in the group (exclude dropouts).</td></tr><tr><td><code>diagnosis</code></td><td>Exact clinical/medical diagnosis, including subtypes and comorbidities.</td></tr><tr><td><code>group_name</code></td><td>Group type: <code>&quot;healthy&quot;</code> (controls) or <code>&quot;patients&quot;</code> (clinical).</td></tr><tr><td><code>subgroup_name</code></td><td>Verbatim group name, if provided.</td></tr><tr><td><code>male_count</code></td><td>Number of males, if explicitly reported.</td></tr><tr><td><code>female_count</code></td><td>Number of females, if explicitly reported.</td></tr><tr><td><code>age_mean</code></td><td>Mean age, if stated directly in the text.</td></tr><tr><td><code>age_range</code></td><td>Age range as stated (e.g., <code>&quot;18-65&quot;</code>); use dash format.</td></tr><tr><td><code>age_minimum</code></td><td>Lowest age reported or lower bound of range.</td></tr><tr><td><code>age_maximum</code></td><td>Highest age reported or upper bound of range.</td></tr><tr><td><code>age_median</code></td><td>Median age, only if explicitly provided.</td></tr></tbody></table><p><strong>Preliminary Validation.</strong> </p><ul><li>We annotated over 220 articles for participant demographics. </li><li>We observed a high level of accuracy for most fields, notably for participant <em>count</em> (\&lt;0.15 Mean Percentage Error). </li><li>In our annotated sample, we identified 100 individual  participant groups with a <em>diagnosis</em> labels (e.g. “schizophrenia”). Using BERTScore to quantitatively compare the extracted and annotated diagnoses, the best performing models achieved &gt;0.8 F1-score, indicating moderate to high accuracy. (higher scores are better). </li><li>Qualitative analysis confirmed that LLMs are increasingly adept at capturing specific diagnostic information (e.g., &quot;Autism Spectrum Disorder&quot;, &quot;phobic prone&quot;, &quot;eating disorders prone&quot;) and associating it correctly with relevant demographic data, even if the specific form differed from the manual annotation. </li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="experimental-details"><strong>Experimental Details</strong><a href="#experimental-details" class="hash-link" aria-label="Direct link to experimental-details" title="Direct link to experimental-details">​</a></h3><p>The goal of this schema was to extract key details of the overall study, and the individual fMRI Tasks that were used. </p><p>For the overall study, the following was extracted:</p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>Modality</code></td><td>Imaging modalities used (e.g., &quot;fMRI-BOLD&quot;, &quot;MEG&quot;, &quot;PET&quot;).</td></tr><tr><td><code>StudyObjective</code></td><td>Brief summary of the study’s main research question or goal.</td></tr></tbody></table><p>For each fMRI task presented within the study, the following was extracted:</p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>TaskName</code></td><td>Exact task name as stated in the text; if not named,  provide brief description.</td></tr><tr><td><code>TaskDescription</code></td><td>1–2 sentence summary of instructions, stimuli, measures, and objectives.</td></tr><tr><td><code>DesignDetails</code></td><td>Detailed design: type, timing, structure, presentation, response methods.</td></tr><tr><td><code>Conditions</code></td><td>All experimental and control conditions mentioned.</td></tr><tr><td><code>TaskMetrics</code></td><td>All measured outcomes: behavioral, neural, and subjective.</td></tr><tr><td><code>Concepts</code></td><td>Specific mental/cognitive concepts explicitly mentioned.</td></tr><tr><td><code>Domain</code></td><td>Primary cognitive domains engaged, if stated.</td></tr><tr><td><code>RestingState</code></td><td>True only if described explicitly as a resting-state scan.</td></tr><tr><td><code>RestingStateMetadata</code></td><td>Rest-specific details: duration, instructions, eyes open/closed, etc.</td></tr><tr><td><code>TaskDesign</code></td><td>Task design type(s): Blocked, EventRelated, Mixed, or Other.</td></tr><tr><td><code>TaskDuration</code></td><td>Total task duration (e.g., &quot;10 minutes&quot; or &quot;600 seconds&quot;).</td></tr></tbody></table><p><strong>Preliminary Validation</strong> </p><p>We annotated 104 papers to validate study/task information, with the majority of these papers sourced from the NeuroVault collection. </p><ul><li><strong>Modality &amp; RestingState:</strong> Modality and Resting State fields demonstrated very high accuracy. For instance, with 94% accuracy for these fields using GPT 4<!-- -->.<!-- -->  </li><li><strong>TaskName and TaskDescription Accuracy:</strong> TaskName is accurate for studies with a clearly defined task name (64/104 of studies), with a score of 0.9 (1-Levenshtein distance). For studies without a clearly defined task name, qualitative review of examples, showed that the models often provided a coherent and plausible description of the task based on the provided context, even if it wasn&#x27;t a direct match to a predefined label.</li></ul><p>This preliminary validation is just a first step. Stay tuned for a more comprehensive evaluation of AI-extracted neuroimaging features<!-- -->!</p><h1><strong>Try AI-Assisted Curation now<!-- -->!</strong></h1><p>These new features are available for you to try out now on <a href="https://compose.neurosynth.org" target="_blank" rel="noopener noreferrer">compose.neurosynth.org</a>.</p><p>Remember, this is an ongoing, iterative effort, and we have many more features planned on the horizon to increase the level of accuracy and transparency of these AI-extracted features. Feel free to suggest features that would be useful for us to extract. </p><p>We&#x27;re always looking for ways to improve Neurosynth Compose, and your feedback is incredibly valuable<!-- -->!<!-- --> If you have thoughts, questions, or suggestions about AI-Assisted Curation or any other feature, please don&#x27;t hesitate to reach out. You can also engage with us and the broader community on <a href="https://neurostars.org/tag/neurosynth-compose" target="_blank" rel="noopener noreferrer">NeuroStars</a>, our discussion forum.</p></div><footer class="row docusaurus-mt-lg blogPostFooterDetailsFull_mRVl"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/compose-docs/blog/tags/neurosynth">neurosynth</a></li></ul></div><div class="col margin-top--sm"><a href="https://github.com/neurostuff/compose-docs/edit/master/blog/2025-6-20-ai-review.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><a class="pagination-nav__link pagination-nav__link--next" href="/compose-docs/blog/2024/1/31/new-year"><div class="pagination-nav__sublabel">Older Post</div><div class="pagination-nav__label">New Year, New Features</div></a></nav></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#large-language-models-llms-for-neuroscientific-information-extraction" class="table-of-contents__link toc-highlight"><strong>Large Language Models (LLMs) for Neuroscientific Information Extraction</strong></a></li><li><a href="#a-revamped-curation-experience" class="table-of-contents__link toc-highlight"><strong>A Revamped Curation Experience</strong></a></li><li><a href="#iterative-approach-to-validation-and-development" class="table-of-contents__link toc-highlight"><strong>Iterative Approach to Validation and Development</strong></a></li><li><a href="#initial-extraction-schemas" class="table-of-contents__link toc-highlight"><strong>Initial Extraction Schemas</strong></a><ul><li><a href="#participant-demographics" class="table-of-contents__link toc-highlight"><strong>Participant Demographics</strong></a></li><li><a href="#experimental-details" class="table-of-contents__link toc-highlight"><strong>Experimental Details</strong></a></li></ul></li></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/compose-docs/">Docs</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://neurostars.org/tag/neuroscout" target="_blank" rel="noopener noreferrer" class="footer__link-item">NeuroStars<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/neurosynth" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/compose-docs/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/neurostuff/neurostore" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Neuroinformatics Lab, Inc.</div></div></div></footer></div>
<script src="/compose-docs/assets/js/runtime~main.97dccc22.js"></script>
<script src="/compose-docs/assets/js/main.559d4219.js"></script>
</body>
</html>