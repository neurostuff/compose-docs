<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.3.1">
<title data-rh="true">AI-Assisted Review | Neurosynth Compose Docs</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://neurostuff.github.io/compose-docs/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://neurostuff.github.io/compose-docs/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://neurostuff.github.io/compose-docs/blog/2025/6/20/ai-review"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="AI-Assisted Review | Neurosynth Compose Docs"><meta data-rh="true" name="description" content="We’re excited to share the release of a significant new innovation in Neurosynth Compose: AI-Assisted Curation. This new feature aims to simplify and accelerate custom neuroimaging meta-analyses, making it easier than ever to curate a set of studies for inclusion into a quantitative meta-analysis."><meta data-rh="true" property="og:description" content="We’re excited to share the release of a significant new innovation in Neurosynth Compose: AI-Assisted Curation. This new feature aims to simplify and accelerate custom neuroimaging meta-analyses, making it easier than ever to curate a set of studies for inclusion into a quantitative meta-analysis."><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2025-06-20T00:00:00.000Z"><meta data-rh="true" property="article:author" content="https://github.com/adelavega"><meta data-rh="true" property="article:tag" content="neurosynth"><link data-rh="true" rel="icon" href="/compose-docs/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://neurostuff.github.io/compose-docs/blog/2025/6/20/ai-review"><link data-rh="true" rel="alternate" href="https://neurostuff.github.io/compose-docs/blog/2025/6/20/ai-review" hreflang="en"><link data-rh="true" rel="alternate" href="https://neurostuff.github.io/compose-docs/blog/2025/6/20/ai-review" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/compose-docs/blog/rss.xml" title="Neurosynth Compose Docs RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/compose-docs/blog/atom.xml" title="Neurosynth Compose Docs Atom Feed"><link rel="stylesheet" href="/compose-docs/assets/css/styles.cce6c2a6.css">
<link rel="preload" href="/compose-docs/assets/js/runtime~main.d08b9596.js" as="script">
<link rel="preload" href="/compose-docs/assets/js/main.559d4219.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/compose-docs/"><div class="navbar__logo"><img src="/compose-docs/img/synth.png" alt="Neurosynth Compose logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/compose-docs/img/synth.png" alt="Neurosynth Compose logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">Neurosynth Compose</b></a><a class="navbar__item navbar__link" href="/compose-docs/">Docs</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/compose-docs/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://compose.neurosynth.org" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Compose Home<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">Recent posts</div><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a aria-current="page" class="sidebarItemLink_mo7H sidebarItemLinkActive_I1ZP" href="/compose-docs/blog/2025/6/20/ai-review">AI-Assisted Review</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/compose-docs/blog/2024/1/31/new-year">New Year, New Features</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/compose-docs/blog/tutorials-updates">New tutorials and updates</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/compose-docs/blog/announcing-ns-compose">Announcing Neurosynth Compose!</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><article itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h1 class="title_f1Hy" itemprop="headline">AI-Assisted Review</h1><div class="container_mt6G margin-vert--md"><time datetime="2025-06-20T00:00:00.000Z" itemprop="datePublished">June 20, 2025</time> · <!-- -->6 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/adelavega" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.com/adelavega.png" alt="Alejandro de la Vega"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/adelavega" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Alejandro de la Vega</span></a></div><small class="avatar__subtitle" itemprop="description">Research Scientist</small></div></div></div></div></header><div id="post-content" class="markdown" itemprop="articleBody"><p>We’re excited to share the release of a significant new innovation in Neurosynth Compose: <strong>AI-Assisted Curation</strong>. This new feature aims to simplify and accelerate custom neuroimaging meta-analyses, making it easier than ever to curate a set of studies for inclusion into a quantitative meta-analysis.</p><p>Traditionally, reviewing studies for inclusion in a meta-analysis takes hundreds of hours, even with a user-friendly interface. The original Neurosynth platform helped by using text mining to group studies by keywords, but extracting crucial details—like participant demographics or task design—remained a challenge because such information is inconsistently reported.</p><p>With Large Language Models (LLMs) and zero-shot learning, we can now automatically extract structured study information directly from full-text articles. This makes <strong>precise, automated neuroimaging meta-analysis</strong> a reality.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="llms-for-neuroscientific-information-extraction"><strong>LLMs for Neuroscientific Information Extraction</strong><a href="#llms-for-neuroscientific-information-extraction" class="hash-link" aria-label="Direct link to llms-for-neuroscientific-information-extraction" title="Direct link to llms-for-neuroscientific-information-extraction">​</a></h2><p>Historically, developing AI models for information extraction required a large number of annotated examples required for training, making it impossible to apply in low-data fields like neuroimaging. However, modern LLMs can interpret scientific text without extensive domain-specific training. It is now possible to  automatically extract information from articles, even in areas where there are very few existing human-labeled examples. This approach, called &quot;<strong>zero-shot learning</strong>&quot;, means modern LLMs can extract information without task-specific training.</p><p>By prompting LLMs carefully, we can extract verifiable details—like sample size, diagnosis, age range, tasks, and modalities—from more than 30,000 studies in the NeuroStore database. These details are displayed during curation, making it easier to compare studies and decide which to include.</p><p><img loading="lazy" alt="Figure 1" src="/compose-docs/assets/images/ai_extraction_workflow-a0d81fd1699210283087eba475584512.png" width="932" height="203" class="img_ev3q">
Figure 1<!-- -->.<!-- --> High-level overview of Zero Shot Information Extraction using LLM prompting</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="a-new-curation-experience"><strong>A New Curation Experience</strong><a href="#a-new-curation-experience" class="hash-link" aria-label="Direct link to a-new-curation-experience" title="Direct link to a-new-curation-experience">​</a></h2><p>This information is presented in two key places within the revamped curation user interface: a <strong>concise table view</strong>, allowing for easy comparison across papers, and more <strong>detailed individual study pages</strong> where you can delve deeper into the extracted specifics for a single paper. </p><p>This supports a faster, PRISMA-compliant screening and eligibility process, all within a web-based interface.</p><p><img loading="lazy" alt="Figure 2" src="/compose-docs/assets/images/ai_table_view-99630e3c0efd157c2784519f63f5cd84.png" width="1124" height="395" class="img_ev3q">
Figure 2<!-- -->.<!-- --> Table view showing AI-extracted information (Task Name, Group names, Diagnosis), across three studies</p><p>By clicking on a row in the table view, you can see detailed study-level extracted information:</p><p><img loading="lazy" alt="Figure 3" src="/compose-docs/assets/images/ai_study_view-9a2466b58b671400c1f84256bb592f1a.png" width="837" height="612" class="img_ev3q">
Figure 3<!-- -->.<!-- --> Detailed study-evel AI-extracted information, showing Participant Demographics.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="validated-iterative-development"><strong>Validated, Iterative Development</strong><a href="#validated-iterative-development" class="hash-link" aria-label="Direct link to validated-iterative-development" title="Direct link to validated-iterative-development">​</a></h2><p>Unlike general-purpose review tools (e.g., Elicit, Perplexity, Google Notebook LM), our approach is grounded in <em>domain expert neuroimaging guidelines</em>. First, we developed expert-driven extraction schemas for neuroimaging. Next, each schema is validated by comparing AI outputs with manual annotations (treated as the gold standard), ensuring accuracy before release. </p><p><img loading="lazy" alt="Figure 4" src="/compose-docs/assets/images/iterative_workflow-1fcdb85d034a4dc3907bf91278f0ad22.png" width="432" height="299" class="img_ev3q"></p><p>Figure 4<!-- -->.<!-- --> Iterative annotation and prompt development workflow. </p><p><strong>This ongoing effort is open to community feedback.</strong> The goal is to continuously refine and check our extraction guidelines for neuroimaging-specific study information that help researchers find and screen studies for inclusion into meta-analysis.</p><p>Annotated studies are sourced from PubMed Central using <em>pubget</em> and annotated using <em>labelbuddy</em>— a set of tools recently introduced by our group for literature mining (Dockes et al., 2024). </p><p>Annotations are openly accessible in the <a href="https://github.com/litmining/labelbuddy-annotations/" target="_blank" rel="noopener noreferrer">labelbuddy annotations GitHub repository</a>, and <a href="https://github.com/neurostuff/neurostore-text-extraction/tree/main/ns_extract/pipelines" target="_blank" rel="noopener noreferrer">extraction pipelines</a> are also made available for transparency.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="initial-extraction-schemas"><strong>Initial Extraction Schemas</strong><a href="#initial-extraction-schemas" class="hash-link" aria-label="Direct link to initial-extraction-schemas" title="Direct link to initial-extraction-schemas">​</a></h2><p>At launch, we support two schemas: <em>participant demographics</em> and <em>experimental details.</em>, extracted from the full text of articles using GPT-4—</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="1-participant-demographics">1. <strong>Participant Demographics</strong><a href="#1-participant-demographics" class="hash-link" aria-label="Direct link to 1-participant-demographics" title="Direct link to 1-participant-demographics">​</a></h3><p>Extracts demopgrahic details separately for each participant group:</p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>count</code></td><td>Total participants in the group (exclude dropouts).</td></tr><tr><td><code>diagnosis</code></td><td>Exact clinical/medical diagnosis, including subtypes and comorbidities.</td></tr><tr><td><code>group_name</code></td><td>Group type: <code>&quot;healthy&quot;</code> (controls) or <code>&quot;patients&quot;</code> (clinical).</td></tr><tr><td><code>subgroup_name</code></td><td>Verbatim group name, if provided.</td></tr><tr><td><code>male_count</code></td><td>Number of males, if explicitly reported.</td></tr><tr><td><code>female_count</code></td><td>Number of females, if explicitly reported.</td></tr><tr><td><code>age_mean</code></td><td>Mean age, if stated directly in the text.</td></tr><tr><td><code>age_range</code></td><td>Age range as stated (e.g., <code>&quot;18-65&quot;</code>); use dash format.</td></tr><tr><td><code>age_minimum</code></td><td>Lowest age reported or lower bound of range.</td></tr><tr><td><code>age_maximum</code></td><td>Highest age reported or upper bound of range.</td></tr><tr><td><code>age_median</code></td><td>Median age, only if explicitly provided.</td></tr></tbody></table><p>Validation on 220 articles showed high accuracy, especially for participant counts (&lt;15% error) and diagnosis classification (&gt;0.8 F1-score using BERTScore, on subset of 100 studies with annotated <em>diagnosis</em>). Qualitative analysis confirmed that LLMs are increasingly adept at capturing specific diagnostic information (e.g., &quot;Autism Spectrum Disorder&quot;, &quot;phobic prone&quot;, &quot;eating disorders prone&quot;) and associating them correctly with relevant demographic data, even if the specific form differed from the manual annotation. </p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="2-experimental-details">2. <strong>Experimental Details</strong><a href="#2-experimental-details" class="hash-link" aria-label="Direct link to 2-experimental-details" title="Direct link to 2-experimental-details">​</a></h3><p>Captures study design and task information:</p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>Modality</code></td><td>Imaging modalities used (e.g., &quot;fMRI-BOLD&quot;, &quot;MEG&quot;, &quot;PET&quot;).</td></tr><tr><td><code>StudyObjective</code></td><td>Brief summary of the study’s main research question or goal.</td></tr></tbody></table><p>For each fMRI task presented within the study, the following was also extracted:</p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>TaskName</code></td><td>Exact task name as stated in the text; if not named,  provide brief description.</td></tr><tr><td><code>TaskDescription</code></td><td>1–2 sentence summary of instructions, stimuli, measures, and objectives.</td></tr><tr><td><code>DesignDetails</code></td><td>Detailed design: type, timing, structure, presentation, response methods.</td></tr><tr><td><code>Conditions</code></td><td>All experimental and control conditions mentioned.</td></tr><tr><td><code>TaskMetrics</code></td><td>All measured outcomes: behavioral, neural, and subjective.</td></tr><tr><td><code>Concepts</code></td><td>Specific mental/cognitive concepts explicitly mentioned.</td></tr><tr><td><code>Domain</code></td><td>Primary cognitive domains engaged, if stated.</td></tr><tr><td><code>RestingState</code></td><td>True only if described explicitly as a resting-state scan.</td></tr><tr><td><code>RestingStateMetadata</code></td><td>Rest-specific details: duration, instructions, eyes open/closed, etc.</td></tr><tr><td><code>TaskDesign</code></td><td>Task design type(s): Blocked, EventRelated, Mixed, or Other.</td></tr><tr><td><code>TaskDuration</code></td><td>Total task duration (e.g., &quot;10 minutes&quot; or &quot;600 seconds&quot;).</td></tr></tbody></table><p>Validation on 104 studies found high accuracy for modality and resting-state fields (94%) and strong performance for task information (1-Levenshtein distance of 0.9), particularly when task names were clearly reported in the original sudies (64% of studies). For studies without a clearly defined task name, qualitative review indicated GPT often provided a coherent and plausible description of the task based on the provided context.</p><p>This preliminary validation is just a first step. Stay tuned for a more comprehensive evaluation of AI-extracted neuroimaging features<!-- -->!</p><h1><strong>Get Started</strong></h1><p>You can try AI-Assisted Curation now at <a href="https://compose.neurosynth.org" target="_blank" rel="noopener noreferrer">compose.neurosynth.org</a>.</p><p>This is an ongoing project: we’ll keep expanding schemas and refining accuracy. We welcome your feedback and ideas—join the conversation on <a href="https://neurostars.org/tag/neurosynth-compose" target="_blank" rel="noopener noreferrer">NeuroStars</a>, our discussion forum.</p></div><footer class="row docusaurus-mt-lg blogPostFooterDetailsFull_mRVl"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/compose-docs/blog/tags/neurosynth">neurosynth</a></li></ul></div><div class="col margin-top--sm"><a href="https://github.com/neurostuff/compose-docs/edit/master/blog/2025-6-20-ai-review.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><a class="pagination-nav__link pagination-nav__link--next" href="/compose-docs/blog/2024/1/31/new-year"><div class="pagination-nav__sublabel">Older Post</div><div class="pagination-nav__label">New Year, New Features</div></a></nav></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#llms-for-neuroscientific-information-extraction" class="table-of-contents__link toc-highlight"><strong>LLMs for Neuroscientific Information Extraction</strong></a></li><li><a href="#a-new-curation-experience" class="table-of-contents__link toc-highlight"><strong>A New Curation Experience</strong></a></li><li><a href="#validated-iterative-development" class="table-of-contents__link toc-highlight"><strong>Validated, Iterative Development</strong></a></li><li><a href="#initial-extraction-schemas" class="table-of-contents__link toc-highlight"><strong>Initial Extraction Schemas</strong></a><ul><li><a href="#1-participant-demographics" class="table-of-contents__link toc-highlight">1. <strong>Participant Demographics</strong></a></li><li><a href="#2-experimental-details" class="table-of-contents__link toc-highlight">2. <strong>Experimental Details</strong></a></li></ul></li></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/compose-docs/">Docs</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://neurostars.org/tag/neuroscout" target="_blank" rel="noopener noreferrer" class="footer__link-item">NeuroStars<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/neurosynth" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/compose-docs/blog">Blog</a></li><li class="footer__item"><a href="https://github.com/neurostuff/neurostore" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Neuroinformatics Lab, Inc.</div></div></div></footer></div>
<script src="/compose-docs/assets/js/runtime~main.d08b9596.js"></script>
<script src="/compose-docs/assets/js/main.559d4219.js"></script>
</body>
</html>